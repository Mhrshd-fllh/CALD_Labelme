{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92xRlbPk7f28"
      },
      "source": [
        "#libraries and pips"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCn7vSZK6nUj",
        "outputId": "65eba72d-8d53-4bf4-9e20-f29e22ed44d6"
      },
      "outputs": [],
      "source": [
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DnYfApFz6Qk0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\falla\\miniconda3\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n"
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8dnIxrT_YKZ"
      },
      "source": [
        "#Preparing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g13gSDrkpOc4",
        "outputId": "f1f1f20e-f83b-45c0-bf3f-b06b88d46573"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 446M/446M [20:02<00:00, 371kiB/s]    \n",
            " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 197M/438M [14:27<17:37, 228kiB/s]     \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error downloading the file.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 841k/1.95G [03:05<119:41:05, 4.52kiB/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error downloading the file.\n"
          ]
        },
        {
          "ename": "BadZipFile",
          "evalue": "File is not a zip file",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 76\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[39mfor\u001b[39;00m url \u001b[39min\u001b[39;00m urls:\n\u001b[0;32m     75\u001b[0m     zip_file \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(destination_dir, os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mbasename(url))\n\u001b[1;32m---> 76\u001b[0m     extract_zip(zip_file, extract_dir)\n\u001b[0;32m     78\u001b[0m \u001b[39m# Define class names\u001b[39;00m\n\u001b[0;32m     79\u001b[0m class_names \u001b[39m=\u001b[39m [\n\u001b[0;32m     80\u001b[0m     \u001b[39m'\u001b[39m\u001b[39maeroplane\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbicycle\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbird\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mboat\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbottle\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbus\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcar\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcat\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     81\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mchair\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcow\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdiningtable\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdog\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhorse\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmotorbike\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mperson\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     82\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mpottedplant\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msheep\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msofa\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtvmonitor\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     83\u001b[0m ]\n",
            "Cell \u001b[1;32mIn[3], line 31\u001b[0m, in \u001b[0;36mextract_zip\u001b[1;34m(zip_file, destination)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract_zip\u001b[39m(zip_file, destination):\n\u001b[1;32m---> 31\u001b[0m     \u001b[39mwith\u001b[39;00m zipfile\u001b[39m.\u001b[39;49mZipFile(zip_file, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m zip_ref:\n\u001b[0;32m     32\u001b[0m         zip_ref\u001b[39m.\u001b[39mextractall(destination)\n",
            "File \u001b[1;32mc:\\Users\\falla\\miniconda3\\Lib\\zipfile.py:1302\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[1;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[0;32m   1300\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1301\u001b[0m     \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m-> 1302\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_RealGetContents()\n\u001b[0;32m   1303\u001b[0m     \u001b[39melif\u001b[39;00m mode \u001b[39min\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m   1304\u001b[0m         \u001b[39m# set the modified flag so central directory gets written\u001b[39;00m\n\u001b[0;32m   1305\u001b[0m         \u001b[39m# even if no files are added to the archive\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_didModify \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\falla\\miniconda3\\Lib\\zipfile.py:1369\u001b[0m, in \u001b[0;36mZipFile._RealGetContents\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1367\u001b[0m     \u001b[39mraise\u001b[39;00m BadZipFile(\u001b[39m\"\u001b[39m\u001b[39mFile is not a zip file\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1368\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m endrec:\n\u001b[1;32m-> 1369\u001b[0m     \u001b[39mraise\u001b[39;00m BadZipFile(\u001b[39m\"\u001b[39m\u001b[39mFile is not a zip file\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1370\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdebug \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1371\u001b[0m     \u001b[39mprint\u001b[39m(endrec)\n",
            "\u001b[1;31mBadZipFile\u001b[0m: File is not a zip file"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "import xml.etree.ElementTree as ET\n",
        "import yaml\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "import zipfile\n",
        "\n",
        "def download_file(url, destination):\n",
        "    response = requests.get(url, stream=True)\n",
        "    total_size = int(response.headers.get('content-length', 0))\n",
        "    block_size = 1024\n",
        "    t = tqdm(total=total_size, unit='iB', unit_scale=True)\n",
        "\n",
        "    with open(destination, 'wb') as file:\n",
        "        for data in response.iter_content(block_size):\n",
        "            t.update(len(data))\n",
        "            file.write(data)\n",
        "\n",
        "    t.close()\n",
        "    if total_size != 0 and t.n != total_size:\n",
        "        print(\"Error downloading the file.\")\n",
        "\n",
        "def download_images(urls, destination):\n",
        "    Path(destination).mkdir(parents=True, exist_ok=True)\n",
        "    for url in urls:\n",
        "        filename = os.path.join(destination, os.path.basename(url))\n",
        "        download_file(url, filename)\n",
        "\n",
        "def extract_zip(zip_file, destination):\n",
        "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "        zip_ref.extractall(destination)\n",
        "\n",
        "def convert_label(path, lb_path, year, image_id, class_names):\n",
        "    def convert_box(size, box):\n",
        "        dw, dh = 1. / size[0], 1. / size[1]\n",
        "        x, y, w, h = (box[0] + box[1]) / 2.0 - 1, (box[2] + box[3]) / 2.0 - 1, box[1] - box[0], box[3] - box[2]\n",
        "        return x * dw, y * dh, w * dw, h * dh\n",
        "\n",
        "    in_file = open(path /'images'/'VOCdevkit/' f'VOC{year}/Annotations/{image_id}.xml')\n",
        "    out_file = open(lb_path, 'w')\n",
        "    tree = ET.parse(in_file)\n",
        "    root = tree.getroot()\n",
        "    size = root.find('size')\n",
        "    w = int(size.find('width').text)\n",
        "    h = int(size.find('height').text)\n",
        "\n",
        "    for obj in root.iter('object'):\n",
        "        cls = obj.find('name').text\n",
        "        if cls in class_names and int(obj.find('difficult').text) != 1:\n",
        "            xmlbox = obj.find('bndbox')\n",
        "            bb = convert_box((w, h), [float(xmlbox.find(x).text) for x in ('xmin', 'xmax', 'ymin', 'ymax')])\n",
        "            cls_id = class_names.index(cls)  # class id\n",
        "            out_file.write(\" \".join(str(a) for a in (cls_id, *bb)) + '\\n')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify the download URLs\n",
        "    urls = [\n",
        "        'https://github.com/ultralytics/yolov5/releases/download/v1.0/VOCtrainval_06-Nov-2007.zip',\n",
        "        'https://github.com/ultralytics/yolov5/releases/download/v1.0/VOCtest_06-Nov-2007.zip',\n",
        "        'https://github.com/ultralytics/yolov5/releases/download/v1.0/VOCtrainval_11-May-2012.zip'\n",
        "    ]\n",
        "\n",
        "    # Specify the destination directory\n",
        "    destination_dir = '/content/datasets/VOC/images'\n",
        "\n",
        "    # Download the files\n",
        "    download_images(urls, destination_dir)\n",
        "\n",
        "    # Specify the destination directory for extracted files\n",
        "    extract_dir = '/content/datasets/VOC/images/'\n",
        "\n",
        "    # Extract the downloaded zip files\n",
        "    for url in urls:\n",
        "        zip_file = os.path.join(destination_dir, os.path.basename(url))\n",
        "        extract_zip(zip_file, extract_dir)\n",
        "\n",
        "    # Define class names\n",
        "    class_names = [\n",
        "        'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat',\n",
        "        'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person',\n",
        "        'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'\n",
        "    ]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2Yx87BdvFJU",
        "outputId": "8a61ca7f-ab89-47df-af5f-380f8bee46aa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "train2012: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5717/5717 [00:02<00:00, 2640.69it/s]\n",
            "val2012: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5823/5823 [00:02<00:00, 2853.19it/s]\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "class_names = [\n",
        "        'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat',\n",
        "        'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person',\n",
        "        'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'\n",
        "    ]\n",
        "# Define the parent directory\n",
        "dir = Path('./datasets/VOC')  # Specify the parent directory\n",
        "\n",
        "# Iterate over each year and image set\n",
        "for year, image_set in [('2012', 'train'), ('2012', 'val')]:\n",
        "    # Define paths for images and labels\n",
        "    imgs_path = dir / 'images' / f'{image_set}{year}'\n",
        "    lbs_path = dir / 'labels' / f'{image_set}{year}'\n",
        "    imgs_path.mkdir(exist_ok=True, parents=True)  # Create images directory if not exists\n",
        "    lbs_path.mkdir(exist_ok=True, parents=True)   # Create labels directory if not exists\n",
        "\n",
        "    # Read image IDs from the text file\n",
        "    with open(dir / 'images' / 'VOCdevkit' / f'VOC{year}' / 'ImageSets' / 'Main' / f'{image_set}.txt') as f:\n",
        "        image_ids = f.read().strip().split()\n",
        "\n",
        "    # Iterate over each image ID\n",
        "    for id in tqdm(image_ids, desc=f'{image_set}{year}'):\n",
        "        # Define old image path and new label path\n",
        "        img_old_path = dir / 'images' / 'VOCdevkit' / f'VOC{year}' / 'JPEGImages' / f'{id}.jpg'\n",
        "        lb_new_path = (lbs_path / f'{id}.txt').with_suffix('.txt')\n",
        "\n",
        "        # Check if the image file exists\n",
        "        if img_old_path.exists():\n",
        "            # Move image to the new directory\n",
        "            os.rename(img_old_path, imgs_path / f'{id}.jpg')\n",
        "\n",
        "            # Convert labels to YOLO format\n",
        "            convert_label(dir, lb_new_path, year, id , class_names)\n",
        "        else:\n",
        "            print(f\"Image file '{img_old_path}' not found. Skipping...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-J6NgtcWu5Xg"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/datasets/VOC_l/images/train2012\n",
        "!mkdir -p /content/datasets/VOC_l/labels/train2012\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "62xYibiPytZn",
        "outputId": "ded7a472-7171-4dbf-b6ac-682f95b3f214"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/datasets/VOC_l/labels/val2012'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import shutil\n",
        "\n",
        "source_dir = \"/content/datasets/VOC/images/val2012\"\n",
        "destination_dir = \"/content/datasets/VOC_l/images/val2012\"\n",
        "shutil.copytree(source_dir, destination_dir)\n",
        "\n",
        "source_dir = \"/content/datasets/VOC/labels/val2012\"\n",
        "destination_dir = \"/content/datasets/VOC_l/labels/val2012\"\n",
        "shutil.copytree(source_dir, destination_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-UGyyH3KZOr"
      },
      "source": [
        "#Augments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fdzaQmOwKbXh"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms.functional as F\n",
        "import torch.nn.functional as Fun\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import PIL\n",
        "from PIL import Image, ImageDraw\n",
        "import datetime\n",
        "import os\n",
        "import time\n",
        "\n",
        "\n",
        "import random\n",
        "import torch\n",
        "\n",
        "from torchvision.transforms import functional as F\n",
        "\n",
        "\n",
        "def _flip_coco_person_keypoints(kps, width):\n",
        "    flip_inds = [0, 2, 1, 4, 3, 6, 5, 8, 7, 10, 9, 12, 11, 14, 13, 16, 15]\n",
        "    flipped_data = kps[:, flip_inds]\n",
        "    flipped_data[..., 0] = width - flipped_data[..., 0]\n",
        "    # Maintain COCO convention that if visibility == 0, then x, y = 0\n",
        "    inds = flipped_data[..., 2] == 0\n",
        "    flipped_data[inds] = 0\n",
        "    return flipped_data\n",
        "\n",
        "\n",
        "class Compose(object):\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        for t in self.transforms:\n",
        "            image, target = t(image, target)\n",
        "        return image, target\n",
        "\n",
        "\n",
        "class RandomHorizontalFlip(object):\n",
        "    def __init__(self, prob):\n",
        "        self.prob = prob\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        if random.random() < self.prob:\n",
        "            height, width = image.shape[-2:]\n",
        "            image = image.flip(-1)\n",
        "            bbox = target[\"boxes\"]\n",
        "            bbox[:, [0, 2]] = width - bbox[:, [2, 0]]\n",
        "            target[\"boxes\"] = bbox\n",
        "            if \"masks\" in target:\n",
        "                target[\"masks\"] = target[\"masks\"].flip(-1)\n",
        "            if \"keypoints\" in target:\n",
        "                keypoints = target[\"keypoints\"]\n",
        "                keypoints = _flip_coco_person_keypoints(keypoints, width)\n",
        "                target[\"keypoints\"] = keypoints\n",
        "        return image, target\n",
        "\n",
        "\n",
        "class ToTensor(object):\n",
        "    def __call__(self, image, target):\n",
        "        image = F.to_tensor(image)\n",
        "        return image, target\n",
        "\n",
        "\n",
        "def HorizontalFlipFeatures(image, features):\n",
        "    image = F.to_tensor(image)\n",
        "    image = image.flip(-1)\n",
        "    new_features = {}\n",
        "    for k in features:\n",
        "        new_features[k] = features[k].detach().flip(-1)\n",
        "    return image, new_features\n",
        "\n",
        "\n",
        "def HorizontalFlip(image, bbox):\n",
        "    import torchvision.transforms.functional as F\n",
        "    from PIL import Image\n",
        "\n",
        "    if isinstance(image, Image.Image):\n",
        "        image = F.to_tensor(image)\n",
        "    height, width = image.shape[-2:]\n",
        "    image = image.flip(-1)\n",
        "    b = bbox.clone()\n",
        "    b[:, [0, 2]] = width - bbox[:, [2, 0]]\n",
        "\n",
        "    # Convert the image back to PIL.Image.Image\n",
        "    image = F.to_pil_image(image)\n",
        "\n",
        "    return image, b\n",
        "\n",
        "\n",
        "def resizeFeatures(img, features, ratio):\n",
        "    if not type(img) == PIL.Image.Image:\n",
        "        img = F.to_pil_image(img)\n",
        "    w, h = img.size\n",
        "    iw = int(w * ratio)\n",
        "    ih = int(h * ratio)\n",
        "    new_features = {}\n",
        "    for k in features:\n",
        "        fw = int(features[k].shape(-2) * ratio)\n",
        "        fh = int(features[k].shape(-1) * ratio)\n",
        "        new_features[k] = Fun.interpolate(features[k].detach(), (fw, fh))\n",
        "    return F.to_tensor(img.resize((iw, ih), Image.BILINEAR)), new_features\n",
        "\n",
        "\n",
        "def resize(image, boxes, ratio):\n",
        "    if isinstance(image, Image.Image):\n",
        "        image = F.to_tensor(image)\n",
        "    h = image.size(1)\n",
        "    w = image.size(2)\n",
        "    # w, h = image.size\n",
        "    ow = int(w * ratio)\n",
        "    oh = int(h * ratio)\n",
        "\n",
        "    image = F.to_pil_image(image)\n",
        "    image = F.to_tensor(image.resize((ow, oh), Image.BILINEAR))\n",
        "    image = F.to_pil_image(image)\n",
        "\n",
        "    return image, boxes * ratio\n",
        "\n",
        "\n",
        "def ColorSwap(image):\n",
        "    perms = ((0, 1, 2), (0, 2, 1), (1, 0, 2),\n",
        "             (1, 2, 0), (2, 0, 1), (2, 1, 0))\n",
        "    swap = perms[random.randint(0, len(perms) - 1)]\n",
        "    image = F.to_tensor(image)\n",
        "    image = image[swap, :, :]\n",
        "    return image\n",
        "\n",
        "\n",
        "def ColorAdjust(image, factor):\n",
        "    image = F.adjust_brightness(image, factor)\n",
        "    image = F.adjust_contrast(image, factor)\n",
        "    image = F.adjust_saturation(image, factor)\n",
        "    return F.to_tensor(image)\n",
        "\n",
        "\n",
        "def GaussianNoise(image, std=1):\n",
        "    image = F.to_tensor(image)\n",
        "    x = image + torch.randn(image.size()) * std / 255.0\n",
        "    return x\n",
        "\n",
        "\n",
        "def SaltPepperNoise(image, prob):\n",
        "    image = F.to_tensor(image)\n",
        "    noise = torch.rand(image.size())\n",
        "    salt = torch.max(image)\n",
        "    pepper = torch.min(image)\n",
        "    image[noise < prob / 2] = salt\n",
        "    image[noise > 1 - prob / 2] = pepper\n",
        "    return image\n",
        "\n",
        "\n",
        "def cutout(image, boxes, cut_num=2, fill_val=0, bbox_remove_thres=0.4, bbox_min_thres=0.1):\n",
        "    '''\n",
        "        Cutout augmentation\n",
        "        image: A PIL image\n",
        "        boxes: bounding boxes, a tensor of dimensions (#objects, 4)\n",
        "        labels: labels of object, a tensor of dimensions (#objects)\n",
        "        fill_val: Value filled in cut out\n",
        "        bbox_remove_thres: Threshold to remove bbox cut by cutout\n",
        "\n",
        "        Out: new image, new_boxes, new_labels\n",
        "    '''\n",
        "    # Convert PIL image to tensor if it's not already\n",
        "    if isinstance(image, Image.Image):\n",
        "        image = F.to_tensor(image)\n",
        "\n",
        "    # Ensure image tensor is on the same device as other tensors\n",
        "    device = boxes.device\n",
        "    image = image.to(device)\n",
        "\n",
        "    original_h = image.size(1)\n",
        "    original_w = image.size(2)\n",
        "    original_channel = image.size(0)\n",
        "\n",
        "    count = 0\n",
        "    for _ in range(50):\n",
        "        # Random cutout size: [0.15, 0.5] of original dimension\n",
        "        cutout_size_h = random.uniform(0.05 * original_h, 0.2 * original_h)\n",
        "        cutout_size_w = random.uniform(0.05 * original_w, 0.2 * original_w)\n",
        "\n",
        "        # Random position for cutout\n",
        "        left = random.uniform(0, original_w - cutout_size_w)\n",
        "        right = left + cutout_size_w\n",
        "        top = random.uniform(0, original_h - cutout_size_h)\n",
        "        bottom = top + cutout_size_h\n",
        "        cutout = torch.FloatTensor([int(left), int(top), int(right), int(bottom)]).to(device)\n",
        "\n",
        "        # Calculate intersect between cutout and bounding boxes\n",
        "        overlap_size = intersect(cutout.unsqueeze(0), boxes)\n",
        "        area_boxes = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n",
        "        ratio = overlap_size / area_boxes\n",
        "        # If all boxes have IoU greater than bbox_remove_thres, try again\n",
        "        if ratio.max().item() > bbox_remove_thres or ratio.max().item() < bbox_min_thres:\n",
        "            continue\n",
        "\n",
        "        cutout_arr = torch.full((original_channel, int(bottom) - int(top), int(right) - int(left)), fill_val).to(device)\n",
        "        image[:, int(top):int(bottom), int(left):int(right)] = cutout_arr\n",
        "        count += 1\n",
        "        if count >= cut_num:\n",
        "            break\n",
        "\n",
        "    # Convert tensor back to PIL image\n",
        "    image = F.to_pil_image(image.cpu())\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms.functional as F\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms.functional as F\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms.functional as F\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def rotate(image, boxes, angle, device='cpu'):\n",
        "    '''\n",
        "        Rotate image and bounding box\n",
        "        image: A PIL image (w, h)\n",
        "        boxes: A tensors of dimensions (#objects, 4)\n",
        "        device: Device to use for the tensors ('cpu' or 'cuda')\n",
        "\n",
        "        Out: rotated image (w, h), rotated boxes\n",
        "    '''\n",
        "    if isinstance(image, Image.Image):\n",
        "        image = F.to_tensor(image).to(device)\n",
        "    if not isinstance(image, Image.Image):\n",
        "        image = F.to_pil_image(image)\n",
        "    new_image = image.copy()\n",
        "    new_boxes = boxes.clone().to(device)\n",
        "\n",
        "    # Rotate image, expand = True\n",
        "    w = image.width\n",
        "    h = image.height\n",
        "    cx = w / 2\n",
        "    cy = h / 2\n",
        "    new_image = new_image.rotate(angle, expand=True)\n",
        "    angle = np.radians(angle)\n",
        "    alpha = np.cos(angle)\n",
        "    beta = np.sin(angle)\n",
        "    # Get affine matrix\n",
        "    AffineMatrix = torch.tensor([[alpha, beta, (1 - alpha) * cx - beta * cy],\n",
        "                                 [-beta, alpha, beta * cx + (1 - alpha) * cy]], device=device)\n",
        "\n",
        "    # Rotation boxes\n",
        "    box_width = (boxes[:, 2] - boxes[:, 0]).reshape(-1, 1)\n",
        "    box_height = (boxes[:, 3] - boxes[:, 1]).reshape(-1, 1)\n",
        "\n",
        "    # Get corners for boxes\n",
        "    x1 = boxes[:, 0].reshape(-1, 1)\n",
        "    y1 = boxes[:, 1].reshape(-1, 1)\n",
        "\n",
        "    x2 = x1 + box_width\n",
        "    y2 = y1\n",
        "\n",
        "    x3 = x1\n",
        "    y3 = y1 + box_height\n",
        "\n",
        "    x4 = boxes[:, 2].reshape(-1, 1)\n",
        "    y4 = boxes[:, 3].reshape(-1, 1)\n",
        "\n",
        "    corners = torch.stack((x1, y1, x2, y2, x3, y3, x4, y4), dim=1)\n",
        "    corners = corners.reshape(-1, 2)  # Tensors of dimension (4* #objects, 2)\n",
        "    corners = corners.to(device)  # Ensure corners are on the same device as AffineMatrix\n",
        "\n",
        "    corners = torch.cat((corners, torch.ones(corners.shape[0], 1, device=device)), dim=1)\n",
        "                        # (Tensors of dimension (4* #objects, 3))\n",
        "\n",
        "    # Ensure AffineMatrix and corners are on the same device\n",
        "    AffineMatrix = AffineMatrix.to(device)\n",
        "\n",
        "    cos = np.abs(AffineMatrix[0, 0].cpu())\n",
        "    sin = np.abs(AffineMatrix[0, 1].cpu())\n",
        "\n",
        "    nW = int((h * sin) + (w * cos))\n",
        "    nH = int((h * cos) + (w * sin))\n",
        "    AffineMatrix[0, 2] += (nW / 2) - cx\n",
        "    AffineMatrix[1, 2] += (nH / 2) - cy\n",
        "\n",
        "    # Apply affine transform\n",
        "    rotate_corners = torch.mm(AffineMatrix.float(), corners.t()).t()\n",
        "    rotate_corners = rotate_corners.reshape(-1, 8)\n",
        "\n",
        "    x_corners = rotate_corners[:, [0, 2, 4, 6]]\n",
        "    y_corners = rotate_corners[:, [1, 3, 5, 7]]\n",
        "\n",
        "    # Get (x_min, y_min, x_max, y_max)\n",
        "    x_min, _ = torch.min(x_corners, dim=1)\n",
        "    x_min = x_min.reshape(-1, 1)\n",
        "    y_min, _ = torch.min(y_corners, dim=1)\n",
        "    y_min = y_min.reshape(-1, 1)\n",
        "    x_max, _ = torch.max(x_corners, dim=1)\n",
        "    x_max = x_max.reshape(-1, 1)\n",
        "    y_max, _ = torch.max(y_corners, dim=1)\n",
        "    y_max = y_max.reshape(-1, 1)\n",
        "\n",
        "    new_boxes = torch.cat((x_min, y_min, x_max, y_max), dim=1)\n",
        "\n",
        "    scale_x = new_image.width / w\n",
        "    scale_y = new_image.height / h\n",
        "\n",
        "    # Resize new image to (w, h)\n",
        "    new_image = new_image.resize((w, h))\n",
        "\n",
        "    # Resize boxes\n",
        "    new_boxes /= torch.Tensor([scale_x, scale_y, scale_x, scale_y])\n",
        "    new_boxes[:, 0] = torch.clamp(new_boxes[:, 0], 0, w)\n",
        "    new_boxes[:, 1] = torch.clamp(new_boxes[:, 1], 0, h)\n",
        "    new_boxes[:, 2] = torch.clamp(new_boxes[:, 2], 0, w)\n",
        "    new_boxes[:, 3] = torch.clamp(new_boxes[:, 3], 0, h)\n",
        "\n",
        "    image = F.to_tensor(new_image).to(device)\n",
        "    image = F.to_pil_image(image)\n",
        "    return image, new_boxes\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def intersect(boxes1, boxes2):\n",
        "    '''\n",
        "        Find intersection of every box combination between two sets of box\n",
        "        boxes1: bounding boxes 1, a tensor of dimensions (n1, 4)\n",
        "        boxes2: bounding boxes 2, a tensor of dimensions (n2, 4)\n",
        "\n",
        "        Out: Intersection each of boxes1 with respect to each of boxes2,\n",
        "             a tensor of dimensions (n1, n2)\n",
        "    '''\n",
        "    device = boxes1.device  # Get the device of boxes1\n",
        "    n1 = boxes1.size(0)\n",
        "    n2 = boxes2.size(0)\n",
        "\n",
        "    # Ensure both tensors are on the same device\n",
        "    boxes1 = boxes1.to(device)\n",
        "    boxes2 = boxes2.to(device)\n",
        "\n",
        "    max_xy = torch.min(boxes1[:, 2:].unsqueeze(1).expand(n1, n2, 2),\n",
        "                       boxes2[:, 2:].unsqueeze(0).expand(n1, n2, 2))\n",
        "\n",
        "    min_xy = torch.max(boxes1[:, :2].unsqueeze(1).expand(n1, n2, 2),\n",
        "                       boxes2[:, :2].unsqueeze(0).expand(n1, n2, 2))\n",
        "    inter = torch.clamp(max_xy - min_xy, min=0)  # (n1, n2, 2)\n",
        "    return inter[:, :, 0] * inter[:, :, 1]  # (n1, n2)\n",
        "\n",
        "\n",
        "# def draw_PIL_image(image, boxes, labels, name, no=None):\n",
        "#     '''\n",
        "#         Draw PIL image\n",
        "#         image: A PIL image\n",
        "#         labels: A tensor of dimensions (#objects,)\n",
        "#         boxes: A tensor of dimensions (#objects, 4)\n",
        "#     '''\n",
        "#     if type(image) != PIL.Image.Image:\n",
        "#         image = F.to_pil_image(image)\n",
        "#     new_image = image.copy()\n",
        "#     labels = labels.tolist()\n",
        "#     draw = ImageDraw.Draw(new_image)\n",
        "#     boxes = boxes.tolist()\n",
        "#     # print(no)\n",
        "#     if no is not None:\n",
        "#         for n in no:\n",
        "#             draw.rectangle(xy=boxes[n], outline='red', width=2)\n",
        "#     else:\n",
        "#         for i in range(len(boxes)):\n",
        "#             draw.rectangle(xy=boxes[i])  # , outline=label_color_map[rev_label_map[labels[i]]])\n",
        "#     new_image.save('vis/{}.jpg'.format(name))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def draw_PIL_image(image, boxes, labels, scores, name):\n",
        "    if type(image) != PIL.Image.Image:\n",
        "        image = F.to_pil_image(image)\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "    plt.gca().set_axis_off()\n",
        "    plt.subplots_adjust(top=1, bottom=0, right=1, left=0,\n",
        "                        hspace=0, wspace=0)\n",
        "    plt.margins(0, 0)\n",
        "    plt.gca().xaxis.set_major_locator(plt.NullLocator())\n",
        "    plt.gca().yaxis.set_major_locator(plt.NullLocator())\n",
        "    # for i in range(len(boxes)):\n",
        "    #     x, y = boxes[i][0], boxes[i][1]\n",
        "    #     w, h = boxes[i][2] - boxes[i][0], boxes[i][3] - boxes[i][1]\n",
        "    #     plt.gca().add_patch(\n",
        "    #         plt.Rectangle((x, y), w, h, fill=False, edgecolor=label_color_map[rev_label_map[labels[i].item()]],\n",
        "    #                       linewidth=2.5))\n",
        "    #     # plt.text(x, y, '{}={}'.format(voc_labels[labels[n]], scores[n]), color='color', verticalalignment='bottom',\n",
        "    #     #              fontsize=4)\n",
        "    plt.savefig('vis/{}.png'.format(name), dpi=256, bbox_inches='tight', pad_inches=0)\n",
        "    # plt.show()\n",
        "    plt.cla()\n",
        "\n",
        "\n",
        "def draw_PIL_image_1(image, ref_boxes, boxes, ref_labels, labels, scores, pm, name, no=None):\n",
        "    if type(image) != PIL.Image.Image:\n",
        "        image = F.to_pil_image(image)\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "    plt.gca().set_axis_off()\n",
        "    plt.subplots_adjust(top=1, bottom=0, right=1, left=0,\n",
        "                        hspace=0, wspace=0)\n",
        "    plt.margins(0, 0)\n",
        "    plt.gca().xaxis.set_major_locator(plt.NullLocator())\n",
        "    plt.gca().yaxis.set_major_locator(plt.NullLocator())\n",
        "    i = 0\n",
        "    if no is not None:\n",
        "        for n in no:\n",
        "            if i < 1:\n",
        "                color = 'green'\n",
        "                x, y = ref_boxes[n][0], ref_boxes[n][1]\n",
        "                w, h = ref_boxes[n][2] - ref_boxes[n][0], ref_boxes[n][3] - ref_boxes[n][1]\n",
        "                plt.text(x, y, '{}={}'.format(voc_labels[ref_labels[n] - 1], round(scores[n].item(), 2)),\n",
        "                         color='white',\n",
        "                         verticalalignment='bottom', bbox={'facecolor': color, 'alpha': 1.0},\n",
        "                         fontsize=24)\n",
        "            else:\n",
        "                color = 'red'\n",
        "                x, y = boxes[n][0], boxes[n][1]\n",
        "                w, h = boxes[n][2] - boxes[n][0], boxes[n][3] - boxes[n][1]\n",
        "                plt.text(x, y + h, '{}={}'.format(voc_labels[labels[n] - 1], round(pm[n].item(), 2)), color='white',\n",
        "                         verticalalignment='bottom', bbox={'facecolor': color, 'alpha': 1.0},\n",
        "                         fontsize=24)\n",
        "            i += 1\n",
        "            plt.gca().add_patch(plt.Rectangle((x, y), w, h, fill=False, edgecolor=color, linewidth=2.5))\n",
        "    plt.savefig('fig/{}'.format(name), dpi=256, bbox_inches='tight', pad_inches=0)\n",
        "    # plt.show()\n",
        "    plt.cla()\n",
        "\n",
        "\n",
        "def draw_PIL_image_2(image, boxes, ref_labels, name, no=None, color='green'):\n",
        "    if type(image) != PIL.Image.Image:\n",
        "        image = F.to_pil_image(image)\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "    plt.gca().set_axis_off()\n",
        "    plt.subplots_adjust(top=1, bottom=0, right=1, left=0,\n",
        "                        hspace=0, wspace=0)\n",
        "    plt.margins(0, 0)\n",
        "    plt.gca().xaxis.set_major_locator(plt.NullLocator())\n",
        "    plt.gca().yaxis.set_major_locator(plt.NullLocator())\n",
        "    i = 0\n",
        "    # if no is not None:\n",
        "    #     for n in no:\n",
        "    #         x, y = boxes[n][0], boxes[n][1]\n",
        "    #         w, h = boxes[n][2] - boxes[n][0], boxes[n][3] - boxes[n][1]\n",
        "    #         plt.gca().add_patch(plt.Rectangle((x, y), w, h, fill=False, edgecolor=color, linewidth=2.5))\n",
        "    plt.savefig('fig/o_{}.eps'.format(name), dpi=256, bbox_inches='tight', pad_inches=0)\n",
        "    # plt.show()\n",
        "    plt.cla()\n",
        "\n",
        "\n",
        "voc_labels = ('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car',\n",
        "              'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n",
        "              'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor')\n",
        "label_map = {k: v + 1 for v, k in enumerate(voc_labels)}\n",
        "# Inverse mapping\n",
        "rev_label_map = {v: k for k, v in label_map.items()}\n",
        "# Colormap for bounding box\n",
        "CLASSES = 20\n",
        "distinct_colors = [\"#\" + ''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n",
        "                   for i in range(CLASSES)]\n",
        "label_color_map = {k: distinct_colors[i] for i, k in enumerate(label_map.keys())}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvsqN5x4DYQq"
      },
      "source": [
        "#Active learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "A4KnKWSIN7Vj"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def calculate_iou(box1, box2):\n",
        "    # Calculate the coordinates of the intersection rectangle\n",
        "    x1 = max(box1[0], box2[0])\n",
        "    y1 = max(box1[1], box2[1])\n",
        "    x2 = min(box1[2], box2[2])\n",
        "    y2 = min(box1[3], box2[3])\n",
        "\n",
        "    # If the intersection is negative (no intersection), return 0\n",
        "    if x2 < x1 or y2 < y1:\n",
        "        return 0.0\n",
        "\n",
        "    # Calculate the area of intersection rectangle\n",
        "    intersection_area = (x2 - x1) * (y2 - y1)\n",
        "\n",
        "    # Calculate the area of both input rectangles\n",
        "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
        "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
        "\n",
        "    # Calculate the Union area by adding areas of both boxes and subtracting the intersection area\n",
        "    union_area = box1_area + box2_area - intersection_area\n",
        "\n",
        "    # Calculate the Intersection over Union (IoU) by dividing the intersection area by the union area\n",
        "    iou = intersection_area / union_area\n",
        "\n",
        "    return iou"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zrCToRY7GeEn"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from scipy.stats import entropy\n",
        "\n",
        "# Precompute the augmented images and boxes once outside the loop\n",
        "def precompute_augmented_images(image_path, original_image_boxes):\n",
        "    flip_image, flip_boxes = HorizontalFlip(image_path, original_image_boxes)\n",
        "    cutout_image = cutout(image_path, original_image_boxes, 2)\n",
        "    resize_image, resize_boxes = resize(image_path, original_image_boxes, 0.8)\n",
        "    rot_image, rot_boxes = rotate(image_path, original_image_boxes, 5)\n",
        "\n",
        "    return [flip_image, cutout_image, resize_image, rot_image], [flip_boxes, original_image_boxes, resize_boxes, rot_boxes]\n",
        "\n",
        "def get_uncertainty(model, image_path):\n",
        "    consistency1 = 0\n",
        "    consistency2 = 0\n",
        "\n",
        "    original_image_results = model(image_path, verbose=False)\n",
        "    if len(original_image_results[0].boxes) == 0:\n",
        "        return 2\n",
        "\n",
        "    original_image_confs = original_image_results[0].boxes.conf\n",
        "    original_image_boxes = original_image_results[0].boxes.xyxy\n",
        "    original_image_labels = original_image_results[0].boxes.cls\n",
        "\n",
        "    augs = ['flip', 'cut_out', 'smaller_resize', 'rotation']\n",
        "\n",
        "    # Precompute augmented images and boxes\n",
        "    augmented_images, augmented_boxes = precompute_augmented_images(image_path, original_image_boxes)\n",
        "\n",
        "    for aug_image, aug_boxes, aug_name in zip(augmented_images, augmented_boxes, augs):\n",
        "        aug_image_results = model(aug_image, verbose=False)\n",
        "        aug_image_confs = aug_image_results[0].boxes.conf\n",
        "        aug_image_boxes = aug_image_results[0].boxes.xyxy\n",
        "\n",
        "        iou_max = 0\n",
        "        for orig_box in aug_boxes:\n",
        "            max_iou = 0\n",
        "            for aug_box in aug_image_boxes:\n",
        "                iou = calculate_iou(orig_box, aug_box)\n",
        "                if iou > max_iou:\n",
        "                    max_iou = iou\n",
        "            iou_max += max_iou\n",
        "        avg_iou_max = iou_max / len(aug_boxes)\n",
        "        consistency1 += avg_iou_max\n",
        "\n",
        "    # Classification uncertainty using Jensen-Shannon Divergence\n",
        "    max_len = max(len(original_image_confs), len(aug_image_confs))\n",
        "    original_image_confs_padded = np.pad(original_image_confs.cpu().numpy(), (0, max_len - len(original_image_confs)), mode='constant')\n",
        "    aug_image_confs_padded = np.pad(aug_image_confs.cpu().numpy(), (0, max_len - len(aug_image_confs)), mode='constant')\n",
        "\n",
        "    # Compute Jensen-Shannon Divergence\n",
        "    p = (original_image_confs_padded + aug_image_confs_padded) / 2.0\n",
        "    js_divergence = (entropy(original_image_confs_padded, p) + entropy(aug_image_confs_padded, p)) / 2.0\n",
        "    consistency2 = 1 - js_divergence\n",
        "\n",
        "    consistency1 /= len(augs)\n",
        "\n",
        "    return consistency1 + consistency2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_kKy1flxpO5"
      },
      "source": [
        "#Training(uncertainty)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVXgYEVTDeg8",
        "outputId": "814420fa-0f3f-4139-b2ea-5ccebe319682"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of images:  500\n",
            "Number of label :  500\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8n.pt to 'yolov8n.pt'...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.23M/6.23M [00:00<00:00, 71.7MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics YOLOv8.1.14 ðŸš€ Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=/content/VOC_2012.yaml, epochs=20, time=None, patience=50, batch=4, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=4, project=None, name=train, exist_ok=False, pretrained=True, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.9, weight_decay=0.0001, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 755k/755k [00:00<00:00, 17.2MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overriding model.yaml nc=80 with nc=20\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 22        [15, 18, 21]  1    755212  ultralytics.nn.modules.head.Detect           [20, [64, 128, 256]]          \n",
            "Model summary: 225 layers, 3014748 parameters, 3014732 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/VOC_l/labels/train2012... 500 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:00<00:00, 1891.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/datasets/VOC_l/labels/train2012.cache\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/VOC_l/labels/val2012... 5823 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5823/5823 [00:03<00:00, 1940.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/datasets/VOC_l/labels/val2012.cache\n",
            "Plotting labels to runs/detect/train/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0001), 63 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train\u001b[0m\n",
            "Starting training for 20 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       1/20     0.709G     0.9705      4.058      1.286         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:18<00:00,  6.77it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:09<00:00, 10.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.761     0.0198     0.0577     0.0393\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       2/20     0.765G      1.011      3.587      1.305         31        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:15<00:00,  8.01it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:06<00:00, 10.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.362      0.125      0.105     0.0667\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       3/20      0.77G      1.103      3.285      1.377         32        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:15<00:00,  8.08it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:06<00:00, 11.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.453      0.184      0.162     0.0986\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       4/20     0.782G      1.161      3.111      1.411         16        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:15<00:00,  8.04it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:08<00:00, 10.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.425      0.203      0.204      0.117\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       5/20     0.782G      1.242      2.975      1.434         20        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:15<00:00,  8.06it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:06<00:00, 10.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.357      0.269      0.225      0.128\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       6/20     0.784G      1.195      2.786      1.441         10        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:17<00:00,  7.20it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:07<00:00, 10.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.322      0.319      0.247      0.139\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       7/20     0.784G      1.258      2.822      1.491         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:15<00:00,  8.02it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:14<00:00,  9.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.304      0.347      0.255      0.144\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       8/20     0.782G      1.162      2.628      1.415         20        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:16<00:00,  7.67it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:10<00:00, 10.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841       0.35      0.367       0.29      0.167\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       9/20     0.782G      1.129      2.487      1.398         14        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:16<00:00,  7.76it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:12<00:00, 10.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841       0.39      0.403      0.349      0.214\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      10/20     0.782G      1.146      2.486      1.431         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:17<00:00,  7.34it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:10<00:00, 10.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.425      0.401       0.37      0.226\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      11/20     0.778G      1.144      2.962       1.46         12        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:16<00:00,  7.50it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:10<00:00, 10.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.402       0.39      0.338      0.198\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      12/20     0.786G      1.101      2.823       1.42          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:15<00:00,  7.82it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:11<00:00, 10.19it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.406      0.399      0.341      0.206\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      13/20     0.789G      1.071      2.655      1.391          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:15<00:00,  7.92it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:10<00:00, 10.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.408      0.416      0.359      0.219\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      14/20     0.784G      1.051      2.595      1.393         15        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:14<00:00,  8.33it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:11<00:00, 10.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.448      0.409      0.378      0.232\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      15/20     0.786G      1.014      2.473      1.359          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:14<00:00,  8.34it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:06<00:00, 10.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.462      0.441      0.408      0.253\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      16/20     0.784G      1.006      2.417      1.331         15        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:15<00:00,  8.19it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:08<00:00, 10.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.469      0.456      0.421      0.262\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      17/20     0.784G     0.9483      2.317      1.256          9        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:15<00:00,  8.31it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:07<00:00, 10.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.496      0.464      0.441      0.282\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      18/20     0.784G     0.9124        2.2      1.255         12        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:15<00:00,  8.12it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:05<00:00, 11.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.513      0.463      0.461      0.298\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      19/20     0.784G     0.8942      2.259      1.241          7        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:14<00:00,  8.38it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:08<00:00, 10.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.519      0.479      0.474      0.307\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      20/20     0.782G     0.8982      2.184      1.264         15        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:14<00:00,  8.55it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:05<00:00, 11.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.521      0.494      0.484      0.313\n",
            "\n",
            "20 epochs completed in 0.482 hours.\n",
            "Optimizer stripped from runs/detect/train/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from runs/detect/train/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating runs/detect/train/weights/best.pt...\n",
            "Ultralytics YOLOv8.1.14 ðŸš€ Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Model summary (fused): 168 layers, 3009548 parameters, 0 gradients, 8.1 GFLOPs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:02<00:00, 11.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.523      0.492      0.484      0.313\n",
            "             aeroplane       5823        433      0.445      0.624      0.516      0.328\n",
            "               bicycle       5823        358      0.412      0.634      0.571      0.381\n",
            "                  bird       5823        559      0.581      0.413      0.457      0.281\n",
            "                  boat       5823        424      0.414      0.377       0.35      0.182\n",
            "                bottle       5823        630      0.418        0.3       0.28       0.18\n",
            "                   bus       5823        301      0.489      0.658      0.613      0.493\n",
            "                   car       5823       1004      0.779      0.541      0.657      0.448\n",
            "                   cat       5823        612      0.726      0.588      0.674      0.411\n",
            "                 chair       5823       1176      0.475      0.409      0.394      0.241\n",
            "                   cow       5823        298      0.408      0.493      0.366      0.242\n",
            "           diningtable       5823        305      0.292      0.511      0.343      0.206\n",
            "                   dog       5823        759      0.743      0.347      0.551      0.365\n",
            "                 horse       5823        360      0.617      0.503      0.549      0.383\n",
            "             motorbike       5823        356      0.449      0.537        0.5      0.314\n",
            "                person       5823       4372      0.855      0.648      0.788      0.503\n",
            "           pottedplant       5823        489      0.288       0.18      0.139     0.0584\n",
            "                 sheep       5823        413      0.332       0.62      0.484      0.317\n",
            "                  sofa       5823        285      0.433      0.425      0.333      0.199\n",
            "                 train       5823        315      0.646      0.616      0.656      0.449\n",
            "             tvmonitor       5823        392      0.659      0.419      0.465      0.287\n",
            "Speed: 0.3ms preprocess, 3.6ms inference, 0.0ms loss, 1.6ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train\u001b[0m\n",
            "Processed 0 files out of 5217\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/scipy/stats/_entropy.py:133: RuntimeWarning: invalid value encountered in divide\n",
            "  pk = 1.0*pk / np.sum(pk, axis=axis, keepdims=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 500 files out of 5217\n",
            "Processed 1000 files out of 5217\n",
            "Processed 1500 files out of 5217\n",
            "Processed 2000 files out of 5217\n",
            "Processed 2500 files out of 5217\n",
            "Processed 3000 files out of 5217\n",
            "Processed 3500 files out of 5217\n",
            "Processed 4000 files out of 5217\n",
            "Processed 4500 files out of 5217\n",
            "Processed 5000 files out of 5217\n",
            "Number of images:  1000\n",
            "Number of label :  1000\n",
            "Ultralytics YOLOv8.1.14 ðŸš€ Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=/content/VOC_2012.yaml, epochs=20, time=None, patience=50, batch=4, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=4, project=None, name=train2, exist_ok=False, pretrained=True, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.9, weight_decay=0.0001, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train2\n",
            "Overriding model.yaml nc=80 with nc=20\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    755212  ultralytics.nn.modules.head.Detect           [20, [64, 128, 256]]          \n",
            "Model summary: 225 layers, 3014748 parameters, 3014732 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train2', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/VOC_l/labels/train2012... 1000 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 2009.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/datasets/VOC_l/labels/train2012.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/VOC_l/labels/val2012.cache... 5823 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5823/5823 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plotting labels to runs/detect/train2/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0001), 63 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train2\u001b[0m\n",
            "Starting training for 20 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       1/20     0.763G     0.9583      3.808      1.288         32        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:35<00:00,  7.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:11<00:00, 10.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.625     0.0692     0.0983     0.0682\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       2/20     0.843G      1.043      3.231      1.347         15        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:33<00:00,  7.54it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:11<00:00, 10.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.543      0.224      0.239      0.154\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       3/20     0.822G      1.161      2.895       1.42         16        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:33<00:00,  7.47it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:12<00:00,  9.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.443      0.316      0.279      0.165\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       4/20     0.824G       1.24      2.802      1.468         20        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:32<00:00,  7.77it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:13<00:00,  9.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.421      0.378      0.326      0.189\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       5/20     0.845G      1.245      2.663      1.473         24        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:34<00:00,  7.24it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:10<00:00, 10.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.403      0.398      0.332      0.194\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       6/20     0.824G      1.213      2.593      1.461         18        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:32<00:00,  7.74it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:13<00:00,  9.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.403      0.402      0.338      0.201\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       7/20     0.824G      1.191      2.521      1.448         12        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:31<00:00,  7.86it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:13<00:00,  9.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841        0.4       0.42      0.364      0.212\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       8/20     0.824G      1.187      2.429       1.45         27        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:34<00:00,  7.29it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:11<00:00, 10.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.471      0.439      0.414      0.247\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       9/20     0.843G      1.117      2.306      1.405         18        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:33<00:00,  7.42it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:11<00:00, 10.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.469      0.424      0.411      0.253\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      10/20     0.822G      1.139      2.316       1.42         23        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:33<00:00,  7.44it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:12<00:00,  9.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.479      0.469      0.446      0.276\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      11/20     0.822G      1.145      2.736      1.471          6        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:32<00:00,  7.73it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:13<00:00,  9.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.499      0.417      0.422       0.26\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      12/20     0.824G      1.133      2.632      1.468          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:32<00:00,  7.70it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:12<00:00, 10.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.504      0.457      0.456      0.285\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      13/20     0.824G      1.059      2.477      1.389          6        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:31<00:00,  7.93it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:12<00:00,  9.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.521      0.471       0.47        0.3\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      14/20     0.824G      1.047      2.365       1.37          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:32<00:00,  7.70it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:12<00:00, 10.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.544       0.48      0.492      0.312\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      15/20     0.824G      1.016      2.323      1.353         13        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:31<00:00,  7.96it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:11<00:00, 10.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.565       0.49      0.506      0.332\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      16/20     0.824G     0.9645      2.195      1.299          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:33<00:00,  7.48it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:10<00:00, 10.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.568      0.505      0.521      0.341\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      17/20     0.824G     0.9621      2.135      1.311         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:32<00:00,  7.81it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:12<00:00, 10.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841       0.57      0.521      0.535      0.352\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      18/20     0.822G     0.9223      2.077      1.265         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:31<00:00,  8.00it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:12<00:00, 10.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.609      0.518       0.55      0.363\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      19/20     0.824G     0.9025      2.015      1.274          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:32<00:00,  7.67it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:09<00:00, 10.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.624      0.525      0.567      0.377\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      20/20     0.822G     0.8807      1.939      1.236          7        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:31<00:00,  7.87it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:12<00:00, 10.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.646      0.522      0.573      0.385\n",
            "\n",
            "20 epochs completed in 0.595 hours.\n",
            "Optimizer stripped from runs/detect/train2/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from runs/detect/train2/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating runs/detect/train2/weights/best.pt...\n",
            "Ultralytics YOLOv8.1.14 ðŸš€ Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Model summary (fused): 168 layers, 3009548 parameters, 0 gradients, 8.1 GFLOPs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:04<00:00, 11.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.646      0.522      0.574      0.385\n",
            "             aeroplane       5823        433      0.858      0.635      0.727      0.499\n",
            "               bicycle       5823        358       0.65      0.617      0.648      0.455\n",
            "                  bird       5823        559      0.666      0.453      0.524      0.327\n",
            "                  boat       5823        424      0.503      0.384      0.381      0.224\n",
            "                bottle       5823        630      0.564      0.421      0.441      0.279\n",
            "                   bus       5823        301      0.747      0.666      0.731      0.589\n",
            "                   car       5823       1004      0.786      0.595      0.697      0.479\n",
            "                   cat       5823        612      0.779      0.657      0.742      0.511\n",
            "                 chair       5823       1176      0.514      0.431      0.447      0.287\n",
            "                   cow       5823        298       0.47      0.527      0.468      0.325\n",
            "           diningtable       5823        305      0.477      0.508      0.445      0.286\n",
            "                   dog       5823        759      0.758      0.409      0.606      0.413\n",
            "                 horse       5823        360      0.708      0.556      0.658      0.458\n",
            "             motorbike       5823        356      0.595      0.596      0.638      0.419\n",
            "                person       5823       4372      0.877      0.632      0.803      0.519\n",
            "           pottedplant       5823        489      0.558      0.202       0.29      0.155\n",
            "                 sheep       5823        413       0.59       0.59      0.586      0.399\n",
            "                  sofa       5823        285      0.503      0.446      0.449      0.303\n",
            "                 train       5823        315      0.675      0.635      0.666      0.452\n",
            "             tvmonitor       5823        392      0.652      0.478      0.525      0.328\n",
            "Speed: 0.3ms preprocess, 4.5ms inference, 0.0ms loss, 1.4ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train2\u001b[0m\n",
            "Processed 0 files out of 4717\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/scipy/stats/_entropy.py:133: RuntimeWarning: invalid value encountered in divide\n",
            "  pk = 1.0*pk / np.sum(pk, axis=axis, keepdims=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 500 files out of 4717\n",
            "Processed 1000 files out of 4717\n",
            "Processed 1500 files out of 4717\n",
            "Processed 2000 files out of 4717\n",
            "Processed 2500 files out of 4717\n",
            "Processed 3000 files out of 4717\n",
            "Processed 3500 files out of 4717\n",
            "Processed 4000 files out of 4717\n",
            "Processed 4500 files out of 4717\n",
            "Number of images:  1500\n",
            "Number of label :  1500\n",
            "Ultralytics YOLOv8.1.14 ðŸš€ Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=/content/VOC_2012.yaml, epochs=20, time=None, patience=50, batch=4, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=4, project=None, name=train3, exist_ok=False, pretrained=True, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.9, weight_decay=0.0001, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train3\n",
            "Overriding model.yaml nc=80 with nc=20\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    755212  ultralytics.nn.modules.head.Detect           [20, [64, 128, 256]]          \n",
            "Model summary: 225 layers, 3014748 parameters, 3014732 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train3', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/VOC_l/labels/train2012... 1500 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1500/1500 [00:01<00:00, 1195.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/datasets/VOC_l/labels/train2012.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/VOC_l/labels/val2012.cache... 5823 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5823/5823 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plotting labels to runs/detect/train3/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0001), 63 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train3\u001b[0m\n",
            "Starting training for 20 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       1/20     0.885G     0.9788      3.703      1.281         34        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [01:16<00:00,  4.93it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:13<00:00,  9.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.612      0.132      0.159       0.11\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       2/20     0.845G      1.107      2.993       1.38         14        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:52<00:00,  7.09it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:14<00:00,  9.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.402      0.341      0.325      0.205\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       3/20     0.845G       1.21      2.789      1.445         15        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:51<00:00,  7.32it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:15<00:00,  9.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.379      0.402      0.327      0.199\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       4/20     0.843G      1.238        2.7      1.483         12        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:51<00:00,  7.32it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:16<00:00,  9.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.421      0.396      0.347      0.202\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       5/20     0.843G      1.254      2.663      1.497         42        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:51<00:00,  7.33it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:16<00:00,  9.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.373      0.334      0.272      0.153\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       6/20     0.843G      1.233      2.612      1.493         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:51<00:00,  7.33it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:14<00:00,  9.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.432      0.413      0.379      0.227\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       7/20     0.843G       1.24      2.509      1.478         13        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:50<00:00,  7.42it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:14<00:00,  9.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.444      0.453      0.415       0.25\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       8/20     0.843G      1.203      2.413      1.449         25        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:51<00:00,  7.33it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:13<00:00,  9.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.511      0.456      0.442      0.268\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       9/20     0.841G       1.17      2.348      1.431         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:52<00:00,  7.18it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:17<00:00,  9.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.494      0.471      0.458      0.283\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      10/20     0.841G      1.144      2.268      1.414         15        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:54<00:00,  6.89it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:16<00:00,  9.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.563      0.467      0.487      0.306\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      11/20     0.837G      1.161      2.609      1.473          9        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:54<00:00,  6.92it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:16<00:00,  9.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841       0.52      0.457      0.467      0.293\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      12/20     0.845G       1.09      2.444      1.407          7        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:52<00:00,  7.10it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:15<00:00,  9.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.543      0.463      0.474      0.296\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      13/20     0.843G      1.112      2.412      1.436          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:52<00:00,  7.16it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:16<00:00,  9.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.575      0.486      0.499      0.324\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      14/20     0.843G      1.051      2.243      1.384          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:52<00:00,  7.19it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:11<00:00, 10.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.579      0.502       0.52      0.339\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      15/20     0.841G      1.031      2.175      1.358          9        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:48<00:00,  7.81it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:14<00:00,  9.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.563      0.506      0.512       0.33\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      16/20     0.841G     0.9987      2.103      1.328         14        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:48<00:00,  7.67it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:13<00:00,  9.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.608      0.525      0.551      0.365\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      17/20     0.839G     0.9569      2.039      1.312          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:48<00:00,  7.70it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:13<00:00,  9.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.622       0.53      0.563      0.375\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      18/20     0.839G     0.9335       1.93      1.281          9        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:48<00:00,  7.71it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:13<00:00,  9.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.634      0.548      0.577      0.384\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      19/20     0.837G     0.9065      1.836      1.261          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:48<00:00,  7.72it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:13<00:00,  9.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.645      0.553      0.588      0.398\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      20/20     0.837G     0.8901      1.822      1.244         10        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:49<00:00,  7.60it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:12<00:00, 10.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.644      0.562      0.597      0.408\n",
            "\n",
            "20 epochs completed in 0.720 hours.\n",
            "Optimizer stripped from runs/detect/train3/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from runs/detect/train3/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating runs/detect/train3/weights/best.pt...\n",
            "Ultralytics YOLOv8.1.14 ðŸš€ Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Model summary (fused): 168 layers, 3009548 parameters, 0 gradients, 8.1 GFLOPs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:06<00:00, 10.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.645      0.562      0.597      0.408\n",
            "             aeroplane       5823        433      0.839      0.654      0.741      0.521\n",
            "               bicycle       5823        358      0.692      0.651        0.7      0.496\n",
            "                  bird       5823        559      0.663       0.52      0.573      0.367\n",
            "                  boat       5823        424      0.602      0.377      0.408      0.246\n",
            "                bottle       5823        630       0.55       0.43      0.447      0.289\n",
            "                   bus       5823        301      0.697      0.704      0.749      0.601\n",
            "                   car       5823       1004      0.769      0.669      0.745      0.517\n",
            "                   cat       5823        612      0.789      0.672       0.75      0.518\n",
            "                 chair       5823       1176      0.561      0.416      0.454      0.286\n",
            "                   cow       5823        298      0.408      0.537      0.433      0.297\n",
            "           diningtable       5823        305      0.548      0.492      0.462       0.29\n",
            "                   dog       5823        759      0.784       0.47      0.652      0.456\n",
            "                 horse       5823        360      0.639      0.581      0.625       0.45\n",
            "             motorbike       5823        356        0.8      0.573      0.694      0.457\n",
            "                person       5823       4372      0.862      0.684      0.808      0.533\n",
            "           pottedplant       5823        489      0.434       0.37      0.329      0.182\n",
            "                 sheep       5823        413      0.506      0.625      0.579      0.411\n",
            "                  sofa       5823        285      0.409      0.572      0.498      0.342\n",
            "                 train       5823        315      0.702      0.679      0.713      0.497\n",
            "             tvmonitor       5823        392      0.639      0.556      0.585        0.4\n",
            "Speed: 0.3ms preprocess, 4.7ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train3\u001b[0m\n",
            "Processed 0 files out of 4217\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/scipy/stats/_entropy.py:133: RuntimeWarning: invalid value encountered in divide\n",
            "  pk = 1.0*pk / np.sum(pk, axis=axis, keepdims=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 500 files out of 4217\n",
            "Processed 1000 files out of 4217\n",
            "Processed 1500 files out of 4217\n",
            "Processed 2000 files out of 4217\n",
            "Processed 2500 files out of 4217\n",
            "Processed 3000 files out of 4217\n",
            "Processed 3500 files out of 4217\n",
            "Processed 4000 files out of 4217\n",
            "Number of images:  2000\n",
            "Number of label :  2000\n",
            "Ultralytics YOLOv8.1.14 ðŸš€ Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=/content/VOC_2012.yaml, epochs=20, time=None, patience=50, batch=4, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=4, project=None, name=train4, exist_ok=False, pretrained=True, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.9, weight_decay=0.0001, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train4\n",
            "Overriding model.yaml nc=80 with nc=20\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    755212  ultralytics.nn.modules.head.Detect           [20, [64, 128, 256]]          \n",
            "Model summary: 225 layers, 3014748 parameters, 3014732 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train4', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/VOC_l/labels/train2012... 2000 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:01<00:00, 1461.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/datasets/VOC_l/labels/train2012.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/VOC_l/labels/val2012.cache... 5823 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5823/5823 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plotting labels to runs/detect/train4/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0001), 63 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train4\u001b[0m\n",
            "Starting training for 20 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       1/20     0.835G     0.9916       3.62      1.297         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [01:12<00:00,  6.94it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:23<00:00,  8.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.501      0.228      0.225      0.154\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       2/20     0.845G      1.112      2.896      1.383         16        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [01:08<00:00,  7.30it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:16<00:00,  9.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.492      0.387       0.39       0.24\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       3/20     0.845G      1.212      2.643      1.447         14        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [01:07<00:00,  7.41it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:13<00:00,  9.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.413      0.422      0.388      0.232\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       4/20     0.845G      1.253      2.709      1.494         14        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [01:07<00:00,  7.38it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:12<00:00, 10.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.439       0.41      0.384      0.234\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       5/20     0.843G      1.257      2.618      1.499         32        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [01:06<00:00,  7.53it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:13<00:00,  9.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.438      0.413      0.386      0.228\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       6/20     0.822G      1.234      2.561      1.479         13        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [01:06<00:00,  7.49it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:13<00:00,  9.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.486      0.448      0.445      0.277\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       7/20     0.822G       1.21      2.444      1.464         10        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [01:07<00:00,  7.39it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:14<00:00,  9.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.468      0.421        0.4       0.24\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       8/20     0.824G      1.187      2.369      1.449          7        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [01:12<00:00,  6.88it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:15<00:00,  9.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.493      0.426      0.428      0.263\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       9/20      0.82G      1.172       2.33       1.43         12        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [01:12<00:00,  6.87it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:14<00:00,  9.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841       0.52      0.484       0.48      0.305\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      10/20     0.818G      1.133       2.22      1.404         12        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [01:11<00:00,  7.04it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:14<00:00,  9.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.544      0.477      0.491       0.31\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      11/20     0.818G      1.177       2.56      1.494          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [01:08<00:00,  7.34it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:12<00:00,  9.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.533      0.475      0.484      0.307\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      12/20     0.822G      1.122      2.387      1.441         14        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [01:06<00:00,  7.51it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:12<00:00, 10.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.576       0.49      0.518      0.333\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      13/20      0.82G      1.107      2.323      1.432          7        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [01:06<00:00,  7.51it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:12<00:00, 10.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.542       0.51      0.517      0.336\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      14/20     0.822G       1.06      2.183      1.382         10        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [01:06<00:00,  7.51it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [01:13<00:00,  9.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       5823      13841      0.571      0.516      0.533      0.351\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      15/20     0.816G      1.034      2.095      1.366          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [01:08<00:00,  7.28it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 727/728 [01:12<00:00, 11.76it/s]"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import numpy as np\n",
        "\n",
        "torch.cuda.set_device(0)\n",
        "random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "cycles = 7\n",
        "model = None  # Initialize the model variable\n",
        "for c in range(cycles):\n",
        "    # model = YOLO(\"yolov8n.pt\")  # build a new model from YAML\n",
        "\n",
        "\n",
        "    # Path to the directory containing images and labels for training\n",
        "    image_dir = \"/content/datasets/VOC/images/train2012\"\n",
        "    label_dir = \"/content/datasets/VOC/labels/train2012\"\n",
        "\n",
        "    # Path to the directory where you want to move the sampled images and labels\n",
        "    output_image_dir = \"/content/datasets/VOC_l/images/train2012\"\n",
        "    output_label_dir = \"/content/datasets/VOC_l/labels/train2012\"\n",
        "\n",
        "    # Number of images to sample\n",
        "    num_samples = 500\n",
        "\n",
        "    # Get a list of all image files in the directory\n",
        "    image_files = os.listdir(image_dir)\n",
        "\n",
        "    # Sample images randomly for the first cycle\n",
        "    if c == 0:\n",
        "        sampled_images = random.sample(image_files, num_samples)\n",
        "    else:\n",
        "        # Dictionary to store the uncertainty scores for each image\n",
        "        uncertainty_scores = {}\n",
        "\n",
        "        # Calculate uncertainty scores for each image\n",
        "        for i, image_file in enumerate(image_files):\n",
        "            image_path = os.path.join(image_dir, image_file)\n",
        "            image = Image.open(image_path)\n",
        "\n",
        "            uncertainty_score = get_uncertainty(model, image)\n",
        "\n",
        "            uncertainty_scores[image_file] = uncertainty_score\n",
        "\n",
        "            # Print progress every 500 files\n",
        "            if i % 500 == 0:\n",
        "                print(f\"Processed {i} files out of {len(image_files)}\")\n",
        "\n",
        "        # Sort the images based on their uncertainty scores\n",
        "        sorted_images = sorted(uncertainty_scores.items(), key=lambda x: x[1])\n",
        "\n",
        "        # Sample num_samples images with the least uncertainty\n",
        "        sampled_images = [image_file for image_file, _ in sorted_images[:num_samples]]\n",
        "\n",
        "    # Move sampled images and their corresponding labels to the output directory\n",
        "    for image_file in sampled_images:\n",
        "        # Move image\n",
        "        image_path = os.path.join(image_dir, image_file)\n",
        "        shutil.move(image_path, output_image_dir)\n",
        "\n",
        "        # Extract corresponding label file name\n",
        "        label_file = image_file.replace(\".jpg\", \".txt\")\n",
        "\n",
        "        # Move label\n",
        "        label_path = os.path.join(label_dir, label_file)\n",
        "        shutil.move(label_path, output_label_dir)\n",
        "\n",
        "    print(\"Number of images: \", len([file for file in os.listdir(output_image_dir) if file.endswith(\".jpg\")]))\n",
        "    print(\"Number of label : \", len([file for file in os.listdir(output_label_dir) if file.endswith(\".txt\")]))\n",
        "\n",
        "    # Train the model\n",
        "    model = YOLO(\"yolov8n.pt\")  # build a new model from YAML\n",
        "    results = model.train(data='/content/VOC_2012.yaml', epochs=20 , batch=4, workers=4,momentum=0.9,weight_decay=0.0001 ,plots=True,optimizer='SGD')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "92xRlbPk7f28"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
